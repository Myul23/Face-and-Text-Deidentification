{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# git clone https://github.com/ultralytics/yolov3\n",
    "# yolov3 folder 내 존재\n",
    "# from utils.general import non_max_suppression\n",
    "# from utils.torch_utils import select_device\n",
    "# from models.common import DetectMultiBackend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모자이크 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mosaic(src, ratio=0.08):\n",
    "    small = cv2.resize(src, None, fx=ratio, fy=ratio, interpolation=cv2.INTER_NEAREST)\n",
    "    return cv2.resize(small, src.shape[:2][::-1], interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mosaic_area(src, area, ratio=0.08):\n",
    "    for i in area:\n",
    "        x = i[0]\n",
    "        y = i[1]\n",
    "        width = i[2]\n",
    "        height = i[3]\n",
    "\n",
    "        # 모자이크 수행\n",
    "        src[y:y + height, x:x + width] = mosaic(src[y:y + height, x:x + width], ratio)\n",
    "\n",
    "    return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 블러 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur(src):\n",
    "    ksize = 30\n",
    "    roi = cv2.blur(src, (ksize, ksize))\n",
    "    return roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur_area(src, area, ratio=0.08):\n",
    "    for i in area:\n",
    "        x = i[0]\n",
    "        y = i[1]\n",
    "        width = i[2]\n",
    "        height = i[3]\n",
    "\n",
    "        src[y:y + height, x:x + width] = blur(src[y:y + height, x:x + width])\n",
    "\n",
    "    return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text용 draw bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r\"Tesseract-OCR/tesseract.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = r'-c tessedit_char_whitelist='+string.ascii_letters\n",
    "conf = r\"-c tessedit_char_whitelist=\" + string.digits\n",
    "conf = conf + string.ascii_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes_on_text(img):\n",
    "    raw_data = pytesseract.image_to_data(img)\n",
    "\n",
    "    print(raw_data)\n",
    "    for count, data in enumerate(raw_data.splitlines()):\n",
    "        if count > 0:\n",
    "            data = data.split()\n",
    "            if len(data) == 12:\n",
    "                x, y, w, h, content = int(data[6]), int(data[7]), int(data[8]), int(data[9]), data[11]\n",
    "                cv2.rectangle(img, (x, y), (w + x, h + y), (255, 255, 255), 2)\n",
    "\n",
    "                roi = img[y : y + h, x : x + w]\n",
    "                roi = cv2.blur(roi, (30, 30))\n",
    "                img[y : y + h, x : x + w] = roi\n",
    "\n",
    "                # cv2.putText(img, content, (x, y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255) , 1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch 연결 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\n",
    "    \"Test/img95.jpg\",\n",
    "    \"Test/img391.jpg\",\n",
    "    \"Test/img561.jpg\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\ultralytics_yolov3_master\n",
      "YOLOv3  2022-5-5 torch 1.7.1+cu110 CUDA:0 (NVIDIA GeForce RTX 3070 Ti, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 261 layers, 61922845 parameters, 0 gradients, 156.1 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "face_model = torch.hub.load(\"ultralytics/yolov3\", \"yolov3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"final/exp/weights/best.pt\"\n",
    "# text_model = torch.load(model_path)\n",
    "device = select_device(0)\n",
    "text_model = DetectMultiBackend(model_path, device=device)\n",
    "\n",
    "img = cv2.imread(images[0])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = np.transpose(img, (2, 0, 1))\n",
    "img = img.reshape(-1, img.shape[0], img.shape[1], img.shape[2])\n",
    "img = np.divide(img, 255)\n",
    "pred = text_model(torch.from_numpy(img))\n",
    "\n",
    "pred = text_model(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = face_model(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 99.46055, 212.78828, 250.88907, 332.35471,   0.91650,   0.00000],\n",
       "         [ 23.37891,   0.77500, 325.43439, 181.12656,   0.44824,  45.00000],\n",
       "         [ 19.90547,   1.44297, 319.02188, 180.99297,   0.35010,  74.00000]], device='cuda:0')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[175.17480, 272.57150, 151.42853, 119.56642,   0.91650,   0.00000],\n",
       "         [174.40665,  90.95078, 302.05548, 180.35156,   0.44824,  45.00000],\n",
       "         [169.46367,  91.21796, 299.11642, 179.55000,   0.35010,  74.00000]], device='cuda:0')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.xywh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = []\n",
    "for per_img in pred.xyxy:\n",
    "    for x, y, x2, y2, _, _ in per_img:\n",
    "        x, y, w, h = map(int, [x, y, x2 - x, y2 - y])\n",
    "        areas.append([x, y, w, h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = cv2.imread(images[0])\n",
    "output = blur_area(frame, areas)\n",
    "# frame = mosaic_area(frame, areas)\n",
    "\n",
    "output = cv2.resize(frame, (frame.shape[0], frame.shape[1]))\n",
    "cv2.imshow(\"test\", output)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### face detect & blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = glob(\"test/faces/images/*\")\n",
    "# images = [\"test/faces/origin/5.jpg\", \"test/faces/origin/2.jpg\"]\n",
    "images = [\n",
    "    \"Test/img95.jpg\",\n",
    "    \"Test/img391.jpg\",\n",
    "    \"Test/img561.jpg\"\n",
    "]\n",
    "# outputs = \"test/blur\"\n",
    "# assert os.path.exists(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\ultralytics_yolov3_master\n",
      "YOLOv3  2022-5-5 torch 1.7.1+cu110 CUDA:0 (NVIDIA GeForce RTX 3070 Ti, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 261 layers, 61922845 parameters, 0 gradients, 156.1 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "face_model = torch.hub.load(\"ultralytics/yolov3\", \"yolov3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!!\n"
     ]
    }
   ],
   "source": [
    "for img in images:\n",
    "    # print(img)\n",
    "    pred = face_model(img)\n",
    "    frame = cv2.imread(img)\n",
    "\n",
    "    areas = []\n",
    "    for per_img in pred.xyxy:\n",
    "        for x, y, x2, y2, _, c in per_img:\n",
    "            # if int(c) != 0:\n",
    "            #     continue\n",
    "            x, y, w, h = map(int, [x, y, x2 - x, y2 - y])\n",
    "            # frame = cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 3)\n",
    "            areas.append([x, y, w, h])\n",
    "\n",
    "    # cv2.imshow(\"test\", frame)\n",
    "\n",
    "    output = blur_area(frame, areas)\n",
    "    cv2.imshow(\"test\", output)\n",
    "    # frame = mosaic_area(frame, areas)\n",
    "\n",
    "    # output = cv2.resize(frame, (frame.shape[0], frame.shape[1]))\n",
    "    # cv2.imshow(\"test\", output)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # output_path = os.path.join(outputs, img.rsplit(\"\\\\\", 1)[1])\n",
    "    output_path = img.rsplit(\"/\", 1)[1]\n",
    "    # cv2.imwrite(output_path, frame)\n",
    "    cv2.imwrite(output_path, output)\n",
    "print(\"finished!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = glob(\"test/images/*\")\n",
    "images = [\"test/images/C.jpg\"]\n",
    "texts = glob(\"test/texts/preds/*.txt\")\n",
    "outputs = \"test/blur\"\n",
    "assert os.path.exists(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!!\n"
     ]
    }
   ],
   "source": [
    "for idx, img_address in enumerate(images):\n",
    "    # print(img_address)\n",
    "    text_address = \"\"\n",
    "    for t in texts:\n",
    "        # if img_address.rsplit(\"\\\\\", 1)[1].rsplit(\".\", 1)[0] == t.rsplit(\"\\\\\", 1)[1].rsplit(\".\", 1)[0]:\n",
    "        if img_address.rsplit(\"/\", 1)[1].rsplit(\".\", 1)[0] == t.rsplit(\"\\\\\", 1)[1].rsplit(\".\", 1)[0]:\n",
    "            text_address = t\n",
    "            break\n",
    "    # print(text_address)\n",
    "    if len(text_address) < 1:\n",
    "        print(\"뭔가 이상해, pred를 가지고 한 건데 pred text가 없대\")\n",
    "        continue\n",
    "\n",
    "    frame = cv2.imread(img_address)\n",
    "    # print(img_address, frame.shape)\n",
    "    # cv2.imshow(\"origin\", frame)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "    # use text detect result\n",
    "    areas = []\n",
    "    with open(text_address, \"r\") as tf:\n",
    "        lines = tf.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.rsplit(\"\\n\", 1)[0].split(\" \")\n",
    "        # print(parts)\n",
    "        cx, cy, w, h = map(float, parts[1:])\n",
    "        x, y, w, h = map(int, [(cx - w / 2) * frame.shape[1], (cy - h / 2) * frame.shape[0], (cx + w / 2) * frame.shape[1], (cy + h / 2) * frame.shape[0]])\n",
    "        # x, y, w, h = map(int, [cx * frame.shape[0], cy * frame.shape[1], w * frame.shape[0], (cy - h / 2) * frame.shape[1]])\n",
    "\n",
    "        # x = max(min(x, frame.shape[0]), 0)\n",
    "        # y = max(min(y, frame.shape[1]), 0)\n",
    "        # if x + w > frame.shape[0]:\n",
    "        #     w = frame.shape[0] - x\n",
    "        # if y + h > frame.shape[1]:\n",
    "        #     y = frame.shape[1] - y\n",
    "\n",
    "        frame = cv2.rectangle(frame, (x, y), (w + h, y + h), (255, 0, 0), 3)\n",
    "        # areas.append([x, y, w, h])\n",
    "    # print(areas)\n",
    "\n",
    "    # output = blur_area(frame, areas)\n",
    "    # frame = mosaic_area(frame, areas)\n",
    "    # output = cv2.resize(output, (500, 500))\n",
    "    # output = cv2.resize(output, (frame.shape[0], frame.shape[1]))\n",
    "    # cv2.imshow(\"test\", output)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "    # output_path = os.path.join(outputs, img_address.rsplit(\"\\\\\", 1)[1])\n",
    "    output_path = os.path.join(outputs, img_address.rsplit(\"/\", 1)[1])\n",
    "    # cv2.imwrite(output_path, output)\n",
    "    cv2.imwrite(output_path, frame)\n",
    "print(\"finished!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = glob(\"test/images/*\")\n",
    "# images = [\"images/F.jpg\", \"images/5.jpg\"]\n",
    "# images = [\"test/images/F.jpg\", \"test/images/A.jpg\"]\n",
    "# images = [\n",
    "#     \"Test/img95.jpg\",\n",
    "#     \"Test/img391.jpg\",\n",
    "#     \"Test/img561.jpg\"\n",
    "# ]\n",
    "texts = glob(\"test/preds/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\ultralytics_yolov3_master\n",
      "YOLOv3  2022-5-5 torch 1.7.1+cu110 CUDA:0 (NVIDIA GeForce RTX 3070 Ti, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 261 layers, 61922845 parameters, 0 gradients, 156.1 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "face_model = torch.hub.load(\"ultralytics/yolov3\", \"yolov3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_address in images:\n",
    "    # print(img_address)\n",
    "    text_address = \"\"\n",
    "    for t in texts:\n",
    "        if img_address.rsplit(\"\\\\\", 1)[1].rsplit(\".\", 1)[0] == t.rsplit(\"\\\\\", 1)[1].rsplit(\".\", 1)[0]:\n",
    "            text_address = t\n",
    "            break\n",
    "    # print(text_address)\n",
    "    if len(text_address) < 1:\n",
    "        print(\"뭔가 이상해, pred를 가지고 한 건데 pred text가 없대\")\n",
    "        break\n",
    "\n",
    "    areas = []\n",
    "    # * face detection\n",
    "    pred = face_model(img_address)\n",
    "    for per_img in pred.xyxy:\n",
    "        for x, y, x2, y2, _, c in per_img:\n",
    "            if int(c) != 0:\n",
    "                continue\n",
    "            x, y, w, h = map(int, [x, y, x2 - x, y2 - y])\n",
    "            areas.append([x, y, w, h])\n",
    "\n",
    "    frame = cv2.imread(img_address)\n",
    "    # cv2.imshow(\"origin\", frame)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "    # * use text detect result\n",
    "    with open(text_address, \"r\") as tf:\n",
    "        lines = tf.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.rsplit(\"\\n\", 1)[0].split(\" \")\n",
    "        cx, cy, w, h = map(float, parts[1:5])\n",
    "        x, y, w, h = map(int, [(cx - w / 2) * frame.shape[0], (cy - h / 2) * frame.shape[1], (cx + w / 2) * frame.shape[0], (cy + h / 2) * frame.shape[1]])\n",
    "\n",
    "        # x, y, w, h = map(int, [cx * frame.shape[0], cy * frame.shape[1], w * frame.shape[0], (cy - h / 2) * frame.shape[1]])\n",
    "\n",
    "        # ? draw (test)\n",
    "        frame = cv2.rectangle(frame, (x, y), (w + h, y + h), (255, 0, 0), 3)\n",
    "        # ? save\n",
    "        # areas.append([x, y, w, h])\n",
    "    # print(areas)\n",
    "\n",
    "    # ? draw (test)\n",
    "    texts = pytesseract.image_to_string(frame)\n",
    "    temp = draw_boxes_on_text(frame)\n",
    "\n",
    "    # ? save\n",
    "    # raw_data = pytesseract.image_to_data(frame)\n",
    "    # for count, data in enumerate(raw_data.splitlines()):\n",
    "    #     if count > 0:\n",
    "    #         data = data.split()\n",
    "    #         if len(data) == 12:\n",
    "    #             x, y, w, h, content = int(data[6]), int(data[7]), int(data[8]), int(data[9]), data[11]\n",
    "    #             areas.append([x, y, w, h])\n",
    "\n",
    "    output = blur_area(temp, areas)\n",
    "    # frame = mosaic_area(frame, areas)\n",
    "\n",
    "    output = cv2.resize(output, (500, 500))\n",
    "    # output = cv2.resize(output, (frame.shape[0], frame.shape[1]))\n",
    "    cv2.imshow(\"output\", output)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('yolo_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f024e45e3aed52a53948853aa7b985e1acbe9a3f4516f10ffa47382c29dbf4df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
